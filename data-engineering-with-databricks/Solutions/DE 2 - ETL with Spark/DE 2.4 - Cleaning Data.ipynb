{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51cf1249-279d-4476-b059-9fcafde6deac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37116d1e-0e1a-4f29-847b-1b52542ec0d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "# Cleaning Data\n",
    "\n",
    "As we inspect and clean our data, we'll need to construct various column expressions and queries to express transformations to apply on our dataset.\n",
    "\n",
    "Column expressions are constructed from existing columns, operators, and built-in functions. They can be used in **`SELECT`** statements to express transformations that create new columns.\n",
    "\n",
    "Many standard SQL query commands (e.g. **`DISTINCT`**, **`WHERE`**, **`GROUP BY`**, etc.) are available in Spark SQL to express transformations.\n",
    "\n",
    "In this notebook, we'll review a few concepts that might differ from other systems you're used to, as well as calling out a few useful functions for common operations.\n",
    "\n",
    "We'll pay special attention to behaviors around **`NULL`** values, as well as formatting strings and datetime fields.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Summarize datasets and describe null behaviors\n",
    "- Retrieve and remove duplicates\n",
    "- Validate datasets for expected counts, missing values, and duplicate records\n",
    "- Apply common transformations to clean and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b03c0f8e-6be9-4ecc-bc52-6be5a952282c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7243c3e3-1c80-4205-b71c-ac8e09440f5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-02.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95a2a4b-b06f-47e5-9e11-df27a4f19be4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Overview\n",
    "\n",
    "We'll work with new users records from the **`users_dirty`** table, which has the following schema:\n",
    "\n",
    "| field | type | description |\n",
    "|---|---|---|\n",
    "| user_id | string | unique identifier |\n",
    "| user_first_touch_timestamp | long | time at which the user record was created in microseconds since epoch |\n",
    "| email | string | most recent email address provided by the user to complete an action |\n",
    "| updated | timestamp | time at which this record was last updated |\n",
    "\n",
    "Let's start by counting values in each field of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa194464-3c18-47bd-8b13-35aabd391ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT count(*), count(user_id), count(user_first_touch_timestamp), count(email), count(updated)\n",
    "FROM users_dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae53c2de-9652-42a2-8e40-2b9027c2151a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Inspect Missing Data\n",
    "\n",
    "Based on the counts above, it looks like there are at least a handful of null values in all of our fields.\n",
    "\n",
    "**NOTE:** Null values behave incorrectly in some math functions, including **`count()`**.\n",
    "\n",
    "- **`count(col)`** skips **`NULL`** values when counting specific columns or expressions.\n",
    "- **`count(*)`** is a special case that counts the total number of rows (including rows that are only **`NULL`** values).\n",
    "\n",
    "We can count null values in a field by filtering for records where that field is null, using either:  \n",
    "**`count_if(col IS NULL)`** or **`count(*)`** with a filter for where **`col IS NULL`**. \n",
    "\n",
    "Both statements below correctly count records with missing emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7336338b-5c04-434b-b3d3-de5712115d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT count_if(email IS NULL) FROM users_dirty;\n",
    "SELECT count(*) FROM users_dirty WHERE email IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a842586f-dbe3-4cee-bb3e-ad2209952ea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "from pyspark.sql.functions import col\n",
    "usersDF = spark.read.table(\"users_dirty\")\n",
    "\n",
    "usersDF.selectExpr(\"count_if(email IS NULL)\")\n",
    "usersDF.where(col(\"email\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bb7485-7994-44da-bb23-3a24d23b724a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " \n",
    "## Deduplicate Rows\n",
    "We can use **`DISTINCT *`** to remove true duplicate records where entire rows contain the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb47c487-c86e-4a4f-9a03-20fbfc699d78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT DISTINCT(*) FROM users_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca5cbd4-d2fc-42fd-9507-0282980b5bff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "usersDF.distinct().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "714576fb-827a-45e9-9eff-c42b753632d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "  \n",
    "## Deduplicate Rows Based on Specific Columns\n",
    "\n",
    "The code below uses **`GROUP BY`** to remove duplicate records based on **`user_id`** and **`user_first_touch_timestamp`** column values. (Recall that these fields are both generated when a given user is first encountered, thus forming unique tuples.)\n",
    "\n",
    "Here, we are using the aggregate function **`max`** as a hack to:\n",
    "- Keep values from the **`email`** and **`updated`** columns in the result of our group by\n",
    "- Capture non-null emails when multiple records are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503fbe04-9041-4c33-9a71-809eb5b377d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW deduped_users AS \n",
    "SELECT user_id, user_first_touch_timestamp, max(email) AS email, max(updated) AS updated\n",
    "FROM users_dirty\n",
    "WHERE user_id IS NOT NULL\n",
    "GROUP BY user_id, user_first_touch_timestamp;\n",
    "\n",
    "SELECT count(*) FROM deduped_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4758fc47-c757-41b5-b87f-c01a30c1e7b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import max\n",
    "dedupedDF = (usersDF\n",
    "    .where(col(\"user_id\").isNotNull())\n",
    "    .groupBy(\"user_id\", \"user_first_touch_timestamp\")\n",
    "    .agg(max(\"email\").alias(\"email\"), \n",
    "         max(\"updated\").alias(\"updated\"))\n",
    "    )\n",
    "\n",
    "dedupedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc40cf7-a4f3-4e96-950a-628b46966c2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's confirm that we have the expected count of remaining records after deduplicating based on distinct **`user_id`** and **`user_first_touch_timestamp`** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62121728-301d-4ab2-83a0-0b0ee77ea443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(DISTINCT(user_id, user_first_touch_timestamp))\n",
    "FROM users_dirty\n",
    "WHERE user_id IS NOT NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1e7764-2936-43d2-bab0-5aba027c4d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "(usersDF\n",
    "    .dropDuplicates([\"user_id\", \"user_first_touch_timestamp\"])\n",
    "    .filter(col(\"user_id\").isNotNull())\n",
    "    .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4108ae96-e899-4475-b4d1-a2b7af65f755",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Validate Datasets\n",
    "Based on our manual review above, we've visually confirmed that our counts are as expected.\n",
    " \n",
    "We can also programmatically perform validation using simple filters and **`WHERE`** clauses.\n",
    "\n",
    "Validate that the **`user_id`** for each row is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e98ca2a-277a-4ae6-bd95-cdf0ce556b20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT max(row_count) <= 1 no_duplicate_ids FROM (\n",
    "  SELECT user_id, count(*) AS row_count\n",
    "  FROM deduped_users\n",
    "  GROUP BY user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62286d7-f7ea-47ea-b40a-b61bd34e194e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "display(dedupedDF\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(count(\"*\").alias(\"row_count\"))\n",
    "    .select((max(\"row_count\") <= 1).alias(\"no_duplicate_ids\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2de78f3-fe02-48f1-b519-f53177a25a4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Confirm that each email is associated with at most one **`user_id`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68d1dc0-8d0f-45b2-8b62-9f841e852d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT max(user_id_count) <= 1 at_most_one_id FROM (\n",
    "  SELECT email, count(user_id) AS user_id_count\n",
    "  FROM deduped_users\n",
    "  WHERE email IS NOT NULL\n",
    "  GROUP BY email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd6986d-34a8-4c1b-b670-e4504adbe287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "display(dedupedDF\n",
    "    .where(col(\"email\").isNotNull())\n",
    "    .groupby(\"email\")\n",
    "    .agg(count(\"user_id\").alias(\"user_id_count\"))\n",
    "    .select((max(\"user_id_count\") <= 1).alias(\"at_most_one_id\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5165016e-0d86-4cc7-aa66-a8c05c7310ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "## Date Format and Regex\n",
    "Now that we've removed null fields and eliminated duplicates, we may wish to extract further value out of the data.\n",
    "\n",
    "The code below:\n",
    "- Correctly scales and casts the **`user_first_touch_timestamp`** to a valid timestamp\n",
    "- Extracts the calendar date and clock time for this timestamp in human readable format\n",
    "- Uses **`regexp_extract`** to extract the domains from the email column using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65058c50-9d45-4ac6-9754-34f496df6144",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *, \n",
    "  date_format(first_touch, \"MMM d, yyyy\") AS first_touch_date,\n",
    "  date_format(first_touch, \"HH:mm:ss\") AS first_touch_time,\n",
    "  regexp_extract(email, \"(?<=@).+\", 0) AS email_domain\n",
    "FROM (\n",
    "  SELECT *,\n",
    "    CAST(user_first_touch_timestamp / 1e6 AS timestamp) AS first_touch \n",
    "  FROM deduped_users\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719e7d88-eaa4-4114-81cf-073af3c3a7f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGRhdGVfZm9ybWF0LCByZWdleHBfZXh0cmFjdAoKZGlzcGxheShkZWR1cGVkREYKICAgIC53aXRoQ29sdW1uKCJmaXJzdF90b3VjaCIsIChjb2woInVzZXJfZmlyc3RfdG91Y2hfdGltZXN0YW1wIikgLyAxZTYpLmNhc3QoInRpbWVzdGFtcCIpKQogICAgLndpdGhDb2x1bW4oImZpcnN0X3RvdWNoX2RhdGUiLCBkYXRlX2Zvcm1hdCgiZmlyc3RfdG91Y2giLCAiTU1NIGQsIHl5eXkiKSkKICAgIC53aXRoQ29sdW1uKCJmaXJzdF90b3VjaF90aW1lIiwgZGF0ZV9mb3JtYXQoImZpcnN0X3RvdWNoIiwgIkhIOm1tOnNzIikpCiAgICAud2l0aENvbHVtbigiZW1haWxfZG9tYWluIiwgcmVnZXhwX2V4dHJhY3QoImVtYWlsIiwgIig/PD1AKS4rIiwgMCkpCik=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1728318451143,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "494f5295-c8fd-44de-a2cf-23eba2cea5fb",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 29.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1728318448310,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1728318448253,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": null,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import date_format, regexp_extract\n",
    "\n",
    "display(dedupedDF\n",
    "    .withColumn(\"first_touch\", (col(\"user_first_touch_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "    .withColumn(\"first_touch_date\", date_format(\"first_touch\", \"MMM d, yyyy\"))\n",
    "    .withColumn(\"first_touch_time\", date_format(\"first_touch\", \"HH:mm:ss\"))\n",
    "    .withColumn(\"email_domain\", regexp_extract(\"email\", \"(?<=@).+\", 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d560ef-f66b-461a-bee6-d7d85e6052cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Profile \n",
    "\n",
    "Databricks version 9.1 and newer offer two convenient methods for data profiling within Notebooks: through the cell output UI and via the dbutils library.\n",
    "\n",
    "When working with data frames or the results of SQL queries in a Databricks Notebook, users have the option to access a dedicated **Data Profile** tab. Clicking on this tab initiates the creation of an extensive data profile, providing not only summary statistics but also histograms that cover the entire dataset, ensuring a comprehensive view of the data, rather than just what is visible.\n",
    "\n",
    "This data profile encompasses a range of insights, including information about numeric, string, and date columns, making it a powerful tool for data exploration and understanding.\n",
    "\n",
    "**Using cell output UI:**\n",
    "\n",
    "1. In the cell output, you will see a `Table` tab on the right.\n",
    "\n",
    "1. Click on the `Table` tab to access the cell output options.\n",
    "\n",
    "1. Next to the `Table` tab, you'll find a \"Data Profile\" tab. Click on it.\n",
    "\n",
    "1. Databricks will automatically execute a new command to generate a data profile.\n",
    "\n",
    "1. The generated data profile will provide summary statistics for numeric, string, and date columns, along with histograms of value distributions for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31843f98-199c-4345-bf4e-9c8c3da12faa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adba750-eae1-4f6f-81f2-bb1a05b38bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4acba77b-fd23-4441-a299-b673ae5896dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DE 2.4 - Cleaning Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
