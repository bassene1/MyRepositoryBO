{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b434028-9fa5-4b87-a352-f706ac146288",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac6a3d96-1ba2-4f0d-92c8-c6268e992856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Extracting Data Directly From Files with Spark SQL\n",
    "\n",
    "In this notebook, you'll learn to extract data directly from files using Spark SQL on Databricks.\n",
    "\n",
    "A number of file formats support this option, but it is most useful for self-describing data formats (such as Parquet and JSON).\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Use Spark SQL to directly query data files\n",
    "- Layer views and CTEs to make referencing data files easier\n",
    "- Leverage **`text`** and **`binaryFile`** methods to review raw file contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de8897b-5c02-4fb5-9065-feaaeed69f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e54cd23-e802-422d-a0c1-1836f95ee0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-02.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7dc05a2-0741-4686-ba40-22d6dc158b57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Data Overview\n",
    "\n",
    "In this example, we'll work with a sample of raw Kafka data written as JSON files. \n",
    "\n",
    "Each file contains all records consumed during a 5-second interval, stored with the full Kafka schema as a multiple-record JSON file.\n",
    "\n",
    "| field | type | description |\n",
    "| --- | --- | --- |\n",
    "| key | BINARY | The **`user_id`** field is used as the key; this is a unique alphanumeric field that corresponds to session/cookie information |\n",
    "| value | BINARY | This is the full data payload (to be discussed later), sent as JSON |\n",
    "| topic | STRING | While the Kafka service hosts multiple topics, only those records from the **`clickstream`** topic are included here |\n",
    "| partition | INTEGER | Our current Kafka implementation uses only 2 partitions (0 and 1) |\n",
    "| offset | LONG | This is a unique value, monotonically increasing for each partition |\n",
    "| timestamp | LONG | This timestamp is recorded as milliseconds since epoch, and represents the time at which the producer appends a record to a partition |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0774bb9-384f-4ac4-bc16-a974192d629c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Note that our source directory contains many JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "388f8fbb-5923-4897-a5b2-a0d9ef8fbe71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(DA.paths.kafka_events)\n",
    "\n",
    "files = dbutils.fs.ls(DA.paths.kafka_events)\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022745c5-f373-4373-aa96-9b8924ee95ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Here, we'll be using relative file paths to data that's been written to the DBFS root. \n",
    "\n",
    "Most workflows will require users to access data from external cloud storage locations. \n",
    "\n",
    "In most companies, a workspace administrator will be responsible for configuring access to these storage locations.\n",
    "\n",
    "Instructions for configuring and accessing these locations can be found in the cloud-vendor specific self-paced courses titled \"Cloud Architecture & Systems Integrations\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20844e75-3796-459c-9734-afc929dd61b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Query a Single File\n",
    "\n",
    "To query the data contained in a single file, execute the query with the following pattern:\n",
    "\n",
    "<strong><code>SELECT * FROM file_format.&#x60;/path/to/file&#x60;</code></strong>\n",
    "\n",
    "Make special note of the use of back-ticks (not single quotes) around the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77fde80-4be0-4616-883a-deb478c61195",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  dbutils.data.summarize(spark.sql(r\"\"\"SELECT * FROM json.`${DA.paths.kafka_events}/001.json` \"\"\"))\nelse:\n  print(\"This DBR version does not support data profiles.\")\n",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1728314635194,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "107cb8ca-6602-443b-8009-ad9999076fa6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 12.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1728314631532,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1728314631481,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": null,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SELECT * FROM json.`${DA.paths.kafka_events}/001.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0bd5d46-32b8-4e29-af3d-616b503007c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Note that our preview displays all 321 rows of our source file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e879c8d-4c54-42a4-a212-20c77876c975",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Query a Directory of Files\n",
    "\n",
    "Assuming all of the files in a directory have the same format and schema, all files can be queried simultaneously by specifying the directory path rather than an individual file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f78452-8900-4520-98cc-e1337596544a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${DA.paths.kafka_events}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e825e6-6fc5-4263-9852-9bed50eadc6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "By default, this query will only show the first  10,000 records or 2 MB, whichever is less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881b4a1f-4bcb-4072-9674-c8bdc59a7163",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Create References to Files\n",
    "This ability to directly query files and directories means that additional Spark logic can be chained to queries against files.\n",
    "\n",
    "When we create a view from a query against a path, we can reference this view in later queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b1b0a7-fd45-44d5-a16b-f3e5bfeafe26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE VIEW event_view\n",
    "AS SELECT * FROM json.`${DA.paths.kafka_events}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecded361-4c22-4b31-8b2c-fb487c05dafb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "As long as a user has permission to access the view and the underlying storage location, that user will be able to use this view definition to query the underlying data. This applies to different users in the workspace, different notebooks, and different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afd6194-db11-4230-b8b3-84fe6c9ea5b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM event_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfdbcc66-a833-435f-8878-a1d8a2be0148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Temporary References to Files\n",
    "\n",
    "Temporary views similarly alias queries to a name that's easier to reference in later queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6c0050-d0a6-41be-b7ef-06c241bd0537",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW events_temp_view\n",
    "AS SELECT * FROM json.`${DA.paths.kafka_events}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88adeb19-b3fe-4542-ae5f-d855bb27bf4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Temporary views exists only for the current SparkSession. On Databricks, this means they are isolated to the current notebook, job, or DBSQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f38d5a-cd02-4ac4-a83d-7f7a5d324c22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM events_temp_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cce3411-4860-47ec-b746-caad21fa58ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Apply CTEs for Reference within a Query \n",
    "Common table expressions (CTEs) are perfect when you want a short-lived, human-readable reference to the results of a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d10b548-390c-42c7-adc4-2c03430f2de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WITH cte_json\n",
    "AS (SELECT * FROM json.`${DA.paths.kafka_events}`)\n",
    "SELECT * FROM cte_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a792cb2-9283-4c68-ae79-84327a23aa2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "CTEs only alias the results of a query while that query is being planned and executed.\n",
    "\n",
    "As such, **the following cell with throw an error when executed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f48b8b6-95eb-4723-be45-08a534f4eedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- SELECT COUNT(*) FROM cte_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bc6359b-d462-45b2-84a6-4e65bdbfbacd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Extract Text Files as Raw Strings\n",
    "\n",
    "When working with text-based files (which include JSON, CSV, TSV, and TXT formats), you can use the **`text`** format to load each line of the file as a row with one string column named **`value`**. This can be useful when data sources are prone to corruption and custom text parsing functions will be used to extract values from text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9928a461-c60d-4549-8d24-ad0944e5ebe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM text.`${DA.paths.kafka_events}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dededff5-0c8f-4384-af53-f46a2cd9c649",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Extract the Raw Bytes and Metadata of a File\n",
    "\n",
    "Some workflows may require working with entire files, such as when dealing with images or unstructured data. Using **`binaryFile`** to query a directory will provide file metadata alongside the binary representation of the file contents.\n",
    "\n",
    "Specifically, the fields created will indicate the **`path`**, **`modificationTime`**, **`length`**, and **`content`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83527fba-ba7b-465e-9275-74e9832f1598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM binaryFile.`${DA.paths.kafka_events}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4b04cb-82ba-4dcd-af5e-2a6337073b90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0098bc95-ce62-4461-be96-f3a4b9687a48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1680f299-da61-4c3c-8417-c64ca957f0da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DE 2.1 - Querying Files Directly",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
